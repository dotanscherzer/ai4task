import axios from 'axios';

export interface LLMResponse {
  bot_message: string;
  topic_number: number;
  next_action: 'ASK' | 'FOLLOW_UP' | 'TOPIC_WRAP' | 'END';
  next_question_text?: string;
  mark_questions_covered?: string[];
  topic_confidence: number;
  covered_points: string[];
  quick_replies: string[];
}

export interface LLMContext {
  currentTopic: number;
  remainingQuestions: string[];
  recentMessages: Array<{ role: string; content: string }>;
  topicState?: {
    confidence: number;
    coveredPoints: string[];
  };
}

const SYSTEM_PROMPT = `××ª×” ×‘×•×˜ ××¨××™×™×Ÿ ×‘×¢×‘×¨×™×ª (RTL) ×¢×‘×•×¨ ×× ×”×œ/×ª ×˜×›× ×•×œ×•×’×™×•×ª. ×”××˜×¨×”: ×œ××¡×•×£ ××™×“×¢ ×œ××¤×ª ××ª×’×¨ ×‘× ×•×©× ×¤×™×¨×•×§ HLD ×œâ€‘Epics/Features/Stories.
×›×œ×œ×™×:
- ×©××œ×” ××—×ª ×‘×›×œ ×¤×¢×. ×§×¦×¨ ×•××§×¦×•×¢×™.
- ×ª××™×“ ×”×¦×¢: ×“×œ×’ / ×œ× ×™×•×“×¢ / ×¢×¦×•×¨ ×•×”××©×š.
- ××•×ª×¨ ×œ×©××•×œ ×©××œ×ª ×”××©×š ××—×ª ×‘×œ×‘×“ ×× ×”×ª×©×•×‘×” ×›×œ×œ×™×ª ××“×™.
- ×× ×”×ª×©×•×‘×” ××›×¡×” ×›×‘×¨ ×©××œ×•×ª × ×•×¡×¤×•×ª ×‘××•×ª×• × ×•×©× â€“ ××œ ×ª×©××œ ××•×ª×Ÿ, ×¡××Ÿ ××•×ª×Ÿ ×‘-mark_questions_covered.
- ××—×¨×™ 2â€“3 ×ª×©×•×‘×•×ª ×˜×•×‘×•×ª ×‘× ×•×©× ××• confidence>=0.7, ×”×©×ª××© ×‘-TOPIC_WRAP ××• ×¢×‘×•×¨ ×œ× ×•×©× ×”×‘× (ASK ×¢× topic_number ×—×“×©).
- **×—×©×•×‘ ×××•×“**: ×”×©×ª××© ×‘-END ×¨×§ ×›××©×¨ ××™×Ÿ ×¢×•×“ ×©××œ×•×ª ×‘×›×œ ×”× ×•×©××™× ×©× ×‘×—×¨×•. ×× ×™×© ×©××œ×•×ª ×©× ×•×ª×¨×• (×’× ×‘× ×•×©× ××—×¨), ×ª××™×“ ×”×©×ª××© ×‘-ASK ××• TOPIC_WRAP.
- ××¡×•×¨ ×œ×‘×§×© ××™×“×¢ ×¨×’×™×©/××–×”×”. ×× ×”××©×ª××© ××¡×¤×§ ××™×“×¢ ×›×–×” â€“ ×‘×§×© ×œ×”×›×œ×™×œ.

×”×—×–×¨ JSON ×‘×œ×‘×“ ×‘×¤×•×¨××˜:
{
  "bot_message": "×˜×§×¡×˜ ×œ×”×¦×’×”",
  "topic_number": 1,
  "next_action": "ASK|FOLLOW_UP|TOPIC_WRAP|END",
  "next_question_text": "×©××œ×” ×”×‘××” ××• ×¨×™×§",
  "mark_questions_covered": ["..."],
  "topic_confidence": 0.0,
  "covered_points": ["..."],
  "quick_replies": ["×”××©×š","×“×œ×’","×œ× ×™×•×“×¢","×¢×¦×•×¨"]
}`;

export class LLMService {
  private apiKey: string;
  private model: string;
  private baseUrl: string;

  constructor() {
    const rawKey = process.env.GEMINI_API_KEY || '';
    this.apiKey = rawKey.trim();
    this.model = process.env.GEMINI_MODEL || 'gemini-1.5-flash';
    this.baseUrl = `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent`;

    if (!this.apiKey) {
      console.warn('âš ï¸ GEMINI_API_KEY not set. LLM features will not work.');
    } else {
      console.log(`âœ… GEMINI_API_KEY loaded successfully (length: ${this.apiKey.length}, prefix: ${this.apiKey.substring(0, 10)}...)`);
      console.log(`   Model: ${this.model}`);
    }
  }

  async getNextAction(
    managerMessage: string,
    action: string,
    context: LLMContext
  ): Promise<LLMResponse | null> {
    if (!this.apiKey) {
      return null; // Fallback mode
    }

    try {
      const userPrompt = this.buildUserPrompt(managerMessage, action, context);
      
      // Combine system prompt with user prompt for Gemini
      const fullPrompt = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const response = await axios.post(
        `${this.baseUrl}?key=${this.apiKey}`,
        {
          contents: [
            {
              parts: [
                { text: fullPrompt }
              ]
            }
          ],
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 2048,
            responseMimeType: 'application/json'
          }
        },
        {
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );

      const content = response.data.candidates?.[0]?.content?.parts?.[0]?.text;
      if (!content) {
        throw new Error('No content in LLM response');
      }

      const parsed = JSON.parse(content) as LLMResponse;
      return parsed;
    } catch (error) {
      console.error('LLM Service Error:', error);
      return null; // Fallback to static questions
    }
  }

  private buildUserPrompt(
    managerMessage: string,
    action: string,
    context: LLMContext
  ): string {
    let prompt = `× ×•×©× × ×•×›×—×™: ${context.currentTopic}\n`;

    if (context.topicState) {
      prompt += `×‘×™×˜×—×•×Ÿ ×‘× ×•×©×: ${context.topicState.confidence}\n`;
      prompt += `× ×§×•×“×•×ª ×©×›×•×¡×•: ${context.topicState.coveredPoints.join(', ')}\n`;
    }

    if (context.remainingQuestions.length > 0) {
      prompt += `×©××œ×•×ª ×©× ×•×ª×¨×• ×‘× ×•×©× ×”× ×•×›×—×™:\n${context.remainingQuestions.map((q, i) => `${i + 1}. ${q}`).join('\n')}\n`;
    } else {
      prompt += `××™×Ÿ ×¢×•×“ ×©××œ×•×ª ×‘× ×•×©× ×”× ×•×›×—×™. ×× ×™×© × ×•×©××™× × ×•×¡×¤×™×, ×¢×‘×•×¨ ××œ×™×”× ×¢× ASK ××• TOPIC_WRAP.\n`;
    }

    prompt += `\n×¤×¢×•×œ×”: ${action}\n`;
    prompt += `×”×•×“×¢×” ××”×× ×”×œ: ${managerMessage}\n\n`;

    prompt += `×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” ××—×¨×•× ×”:\n`;
    context.recentMessages.slice(-8).forEach((msg) => {
      prompt += `${msg.role}: ${msg.content}\n`;
    });

    prompt += `\n×”×¢×¨×” ×—×©×•×‘×”: ×”××¢×¨×›×ª ×‘×•×“×§×ª ××•×˜×•××˜×™×ª ×× ×™×© ×©××œ×•×ª × ×•×¡×¤×•×ª ×‘×›×œ ×”× ×•×©××™× ×œ×¤× ×™ ×¡×™×•×. ×”×©×ª××© ×‘-END ×¨×§ ×× ××ª×” ×‘×˜×•×— ×©××™×Ÿ ×¢×•×“ ×©××œ×•×ª ×¨×œ×•×•× ×˜×™×•×ª. ×× ×™×© ×¡×¤×§, ×”×©×ª××© ×‘-ASK ××• TOPIC_WRAP.`;
    prompt += `\n×–×›×•×¨: ××œ ×ª×‘×§×© ××™×“×¢ ×¨×’×™×©/××–×”×”. ×× ×”××©×ª××© ××¡×¤×§ ××™×“×¢ ×›×–×” â€“ ×‘×§×© ×œ×”×›×œ×™×œ.`;

    return prompt;
  }

  async generateQuestionsForChallenge(
    challengeName: string,
    challengeDescription: string,
    topic: { number: number; label: string; description: string }
  ): Promise<string[]> {
    console.log(`   ğŸ”§ LLM Service: Starting question generation for topic ${topic.number}`);
    if (this.apiKey) {
      console.log(`      API Key: SET (length: ${this.apiKey.length}, prefix: ${this.apiKey.substring(0, 10)}...)`);
    } else {
      console.log(`      API Key: NOT SET`);
    }
    console.log(`      Model: ${this.model}`);
    console.log(`      Base URL: ${this.baseUrl}`);
    
    if (!this.apiKey) {
      console.warn(`   âš ï¸ GEMINI_API_KEY not set. Cannot generate questions for topic ${topic.number}.`);
      return [];
    }

    try {
      const prompt = this.buildQuestionGenerationPrompt(challengeName, challengeDescription, topic);
      
      // Combine system instruction with prompt for Gemini
      const systemInstruction = '××ª×” ×¢×•×–×¨ ×œ×™×¦×•×¨ ×©××œ×•×ª ×× ×—×•×ª ×¢×‘×•×¨ ×¨×™××™×•×Ÿ. ×”×—×–×¨ ×¨×§ JSON ×¢× ××¢×¨×š ×©×œ 3-4 ×©××œ×•×ª ×‘×¢×‘×¨×™×ª.';
      const fullPrompt = `${systemInstruction}\n\n${prompt}`;
      
      console.log(`   ğŸ“¤ Sending request to Gemini...`);
      console.log(`      Prompt length: ${fullPrompt.length} characters`);
      console.log(`      Challenge name: ${challengeName.substring(0, 50)}...`);
      console.log(`      Topic: ${topic.number} - ${topic.label}`);

      const startTime = Date.now();
      const response = await axios.post(
        `${this.baseUrl}?key=${this.apiKey}`,
        {
          contents: [
            {
              parts: [
                { text: fullPrompt }
              ]
            }
          ],
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 1024,
            responseMimeType: 'application/json'
          }
        },
        {
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );
      const duration = Date.now() - startTime;
      console.log(`   â±ï¸ LLM request completed in ${duration}ms`);
      console.log(`   ğŸ“¥ Response status: ${response.status}`);

      const content = response.data.candidates?.[0]?.content?.parts?.[0]?.text;
      if (!content) {
        console.error(`   âŒ No content in LLM response`);
        console.error(`      Response data: ${JSON.stringify(response.data).substring(0, 200)}...`);
        throw new Error('No content in LLM response');
      }

      console.log(`   ğŸ“„ Response content length: ${content.length} characters`);
      console.log(`   ğŸ“„ Response preview: ${content.substring(0, 100)}...`);

      let parsed: any;
      try {
        parsed = JSON.parse(content);
        console.log(`   âœ… Successfully parsed JSON response`);
      } catch (parseError: any) {
        console.error(`   âŒ Failed to parse JSON response`);
        console.error(`      Parse error: ${parseError.message}`);
        console.error(`      Content preview: ${content.substring(0, 500)}...`);
        throw new Error(`Failed to parse LLM response as JSON: ${parseError.message}`);
      }
      
      // Handle different response formats
      let questions: string[] = [];
      console.log(`   ğŸ” Parsing response structure...`);
      console.log(`      Response keys: ${Object.keys(parsed).join(', ')}`);
      
      if (parsed.questions && Array.isArray(parsed.questions)) {
        questions = parsed.questions;
        console.log(`   âœ… Found questions in 'questions' array (${questions.length} items)`);
      } else if (parsed.questionList && Array.isArray(parsed.questionList)) {
        questions = parsed.questionList;
        console.log(`   âœ… Found questions in 'questionList' array (${questions.length} items)`);
      } else if (Array.isArray(parsed)) {
        questions = parsed;
        console.log(`   âœ… Response is direct array (${questions.length} items)`);
      } else {
        // Try to extract questions from any array in the response
        const values = Object.values(parsed);
        const firstArray = values.find((v) => Array.isArray(v)) as string[] | undefined;
        if (firstArray) {
          questions = firstArray;
          console.log(`   âœ… Found array in response values (${questions.length} items)`);
        } else {
          console.warn(`   âš ï¸ No array found in response structure`);
          console.warn(`      Response structure: ${JSON.stringify(parsed).substring(0, 300)}...`);
        }
      }

      // Validate and filter questions
      const originalCount = questions.length;
      questions = questions
        .filter((q: any) => typeof q === 'string' && q.trim().length > 0)
        .map((q: string) => q.trim())
        .slice(0, 4); // Max 4 questions

      if (originalCount !== questions.length) {
        console.log(`   ğŸ” Filtered questions: ${originalCount} â†’ ${questions.length} (removed invalid/empty)`);
      }

      // If we got less than 3 questions, try to generate more or return what we have
      if (questions.length < 3 && questions.length > 0) {
        console.warn(`   âš ï¸ Generated only ${questions.length} questions for topic ${topic.number} (expected 3-4)`);
      }

      if (questions.length > 0) {
        console.log(`   âœ… Successfully extracted ${questions.length} questions`);
        questions.forEach((q, i) => {
          console.log(`      ${i + 1}. ${q.substring(0, 60)}${q.length > 60 ? '...' : ''}`);
        });
      } else {
        console.warn(`   âš ï¸ No valid questions extracted from LLM response`);
      }

      return questions.length > 0 ? questions : [];
    } catch (error: any) {
      console.error(`   âŒ LLM Question Generation Error for topic ${topic.number}:`);
      console.error(`      Error type: ${error.constructor?.name || 'Unknown'}`);
      console.error(`      Error message: ${error.message || error}`);
      if (error.response) {
        console.error(`      Response status: ${error.response.status}`);
        console.error(`      Response data: ${JSON.stringify(error.response.data).substring(0, 300)}...`);
      }
      if (error.stack) {
        console.error(`      Stack trace: ${error.stack.substring(0, 300)}...`);
      }
      return [];
    }
  }

  private buildQuestionGenerationPrompt(
    challengeName: string,
    challengeDescription: string,
    topic: { number: number; label: string; description: string }
  ): string {
    return `×¦×•×¨ 3-4 ×©××œ×•×ª ×× ×—×•×ª ×‘×¢×‘×¨×™×ª ×¢×‘×•×¨ ×¨×™××™×•×Ÿ ×‘× ×•×©× ×¤×™×¨×•×§ HLD ×œ-Epics/Features/Stories.

×¤×¨×˜×™ ×”××ª×’×¨:
×©×: ${challengeName}
×ª×™××•×¨: ${challengeDescription}

× ×•×©×:
××¡×¤×¨: ${topic.number}
×›×•×ª×¨×ª: ${topic.label}
×ª×™××•×¨: ${topic.description}

×”× ×—×™×•×ª:
- ×”×©××œ×•×ª ×¦×¨×™×›×•×ª ×œ×”×™×•×ª ×¨×œ×•×•× ×˜×™×•×ª ×œ××ª×’×¨ ×”×¡×¤×¦×™×¤×™ ×•×œ×ª××™× ×œ× ×•×©×
- ×”×©××œ×•×ª ×¦×¨×™×›×•×ª ×œ×”×™×•×ª ×§×¦×¨×•×ª, ××§×¦×•×¢×™×•×ª ×•×× ×—×•×ª
- ×”×©××œ×•×ª ×¦×¨×™×›×•×ª ×œ×”×™×•×ª ×‘×¢×‘×¨×™×ª
- ×”×©××œ×•×ª ×¦×¨×™×›×•×ª ×œ×¢×–×•×¨ ×œ×”×‘×™×Ÿ ××ª ×”××ª×’×¨ ×× ×§×•×“×ª ×”××‘×˜ ×©×œ ×”× ×•×©×
- ×›×œ ×©××œ×” ×¦×¨×™×›×” ×œ×”×™×•×ª ×©×•× ×” ×•×—×“×©×”

×”×—×–×¨ JSON ×‘×¤×•×¨××˜:
{
  "questions": ["×©××œ×” 1", "×©××œ×” 2", "×©××œ×” 3", "×©××œ×” 4"]
}

×”×—×–×¨ ×‘×“×™×•×§ 3-4 ×©××œ×•×ª.`;
  }
}

export const llmService = new LLMService();

